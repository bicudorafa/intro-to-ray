{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682e224e-1bb9-470c-b363-386ede0785a4",
   "metadata": {},
   "source": [
    "# Introduction to Ray Serve with PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55345a83",
   "metadata": {},
   "source": [
    "ðŸ’» **Launch Locally**: You can run this notebook locally, but performance will be reduced.\n",
    "\n",
    "ðŸš€ **Launch on Cloud**: A Ray Cluster (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1060aea0",
   "metadata": {},
   "source": [
    "This notebook will introduce you to Ray Serve with PyTorch, a framework for building and deploying scalable ML applications.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Here is the roadmap for this notebook:</b>\n",
    "\n",
    "<ul>\n",
    "    <li><b>1.</b> When to consider Ray Serve</li>\n",
    "    <li><b>2.</b> Overview of Ray Serve</li>\n",
    "    <li><b>3.</b> Implement an image classification service</li>\n",
    "    <li><b>4.</b> Development workflow with Ray Serve </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d528147",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099b7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from typing import Any\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from ray import serve\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b34936",
   "metadata": {},
   "source": [
    "## 1. When to Consider Ray Serve\n",
    "\n",
    "Consider using Ray Serve for your project if it meets one or more of the following criteria:\n",
    "\n",
    "| **Challenge** | **Details** | **Ray Serve Solution** |\n",
    "|---------------|------------------|--------------------------|\n",
    "| **Slow iteration speed for ML engineers** | - Developers need to containerize and rollout components on Kubernetes to test changes<br>- Developers need to use complex protocols (e.g. gRPC) to achieve acceptable performance | - Provides a Python-first API to develop lightweight services<br>- Services are lightweight [Ray actors](https://docs.ray.io/en/latest/ray-core/actors.html)<br>- Ray Serve can be run locally for development |\n",
    "| **Need to efficiently compose multiple components** | - Requires efficient data sharing between components<br>- Implementing performant streaming protocols (e.g. gRPC) is a complex task | - Relies on [Ray's object store](https://docs.ray.io/en/latest/ray-core/objects.html) to share data optimally<br>- Avoids the need to implement gRPC streaming |\n",
    "| **Poor utilization of expensive hardware** | Suffering from poor utilization due to naive request handling | - Offers [dynamic batching of requests](https://docs.ray.io/en/latest/serve/advanced-guides/dyn-req-batch.html) to improve hardware utilization<br>- Leverages Ray Core's support for accelerators and custom resources:<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ [Multi-node/multi-GPU serving](https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html)<br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ [Fractional compute resource usage](https://docs.ray.io/en/latest/serve/configure-serve-deployment.html)<br>- RayTurbo Serve offers [replica compaction](https://www.anyscale.com/blog/new-feature-replica-compaction?_gl=1*lrhlou*_gcl_au*OTY4NjkwODIzLjE3Mzg1Mjc2MzA.) |\n",
    "| **High-latency outliers when juggling many models** | Stuck with naive load balancing and expensive state loading (e.g. ML models) | - Provides [model multiplexing](https://docs.ray.io/en/latest/serve/model-multiplexing.html) to avoid unnecessary load times<br>- Routes to replicas that already have a model loaded |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250cc03-e52c-4e30-a262-8d8e0a5a0837",
   "metadata": {},
   "source": [
    "## 2. Overview of Ray Serve\n",
    "\n",
    "Serve is a framework for serving ML applications. \n",
    "\n",
    "### Applications\n",
    "\n",
    "Here is a high-level overview of the architecture of a Ray Serve Application.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/serve_architecture.png' width=700/>\n",
    "\n",
    "A Ray Serve cluster is made up of one or more Applications.\n",
    "\n",
    "An Application is composed of one or more Deployments that work together. Key characteristics:\n",
    "- Applications are coarse-grained units of functionality\n",
    "- They can be **independently upgraded** without affecting other applications running on the same cluster\n",
    "- They provide isolation and separate deployment lifecycles\n",
    "\n",
    "### Deployments\n",
    "\n",
    "A Deployment is the fundamental building block in Ray Serve's architecture.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment.png' width=600/>\n",
    "\n",
    "Deployments enable:\n",
    "- Separation of concerns (e.g., different models, business logic, data transformations)\n",
    "- **Independent scaling**, including autoscaling capabilities\n",
    "- Multiple replicas for handling concurrent requests\n",
    "\n",
    "\n",
    "### Replicas\n",
    "Each Replica is a worker process (Ray actor) with its own request processing queue. Replicas offer flexible configuration options:\n",
    "\n",
    "- Specify its own hardware and resource requirements (e.g., GPUs)\n",
    "- Specify its own runtime environments (e.g., libraries)\n",
    "- Maintain state (e.g., models)\n",
    "\n",
    "This architecture provides a clean separation of concerns while enabling high scalability and efficient resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43da1a6",
   "metadata": {},
   "source": [
    "## 3. Implement an image classification service\n",
    "\n",
    "Letâ€™s jump right in and get a simple ML service up and running on Ray Serve. \n",
    "\n",
    "Here is an image classification service that performs inference on a batch of handwritten digits using an `MNISTClassifier` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14fb17a6-a71c-4a11-8ea8-b1b350a5fa1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTClassifier:\n",
    "    def __init__(self, remote_path: str, local_path: str, device: str):\n",
    "        subprocess.run(f\"aws s3 cp {remote_path} {local_path} --no-sign-request\", shell=True, check=True)\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = torch.jit.load(local_path).to(device).eval()\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        return self.predict(batch)\n",
    "    \n",
    "    def predict(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d148b8",
   "metadata": {},
   "source": [
    "First we need to load the classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a79961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt to ../../../mnt/cluster_storage/model.pt\n"
     ]
    }
   ],
   "source": [
    "storage_folder = '/mnt/cluster_storage'  # Modify this path to your local folder if it runs on your local environment\n",
    "model_path = f\"{storage_folder}/model.pt\" # Use your local path\n",
    "classifier = MNISTClassifier(remote_path=\"s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt\", local_path=model_path, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b7dbc",
   "metadata": {},
   "source": [
    "Then we can run inference to generate predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b16400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = classifier({\"image\": np.random.rand(1, 1, 28, 28).astype(np.float32)})  # Example input (B, C, H, W)\n",
    "output[\"predicted_label\"]  # Should be a numpy array with the predicted label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1a687",
   "metadata": {},
   "source": [
    "Now, if we want to migrate to an online inference setting, we can transform this into a Ray Serve Deployment by applying the `@serve.deployment` decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68888dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-11 03:09:26,889\tWARNING api.py:346 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2026-02-11 03:09:26,890\tWARNING api.py:397 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n"
     ]
    }
   ],
   "source": [
    "@serve.deployment() # this is the decorator to add\n",
    "class OnlineMNISTClassifier:\n",
    "    # same code as MNISTClassifier.__init__\n",
    "    def __init__(self, remote_path: str, local_path: str, device: str):\n",
    "        subprocess.run(f\"aws s3 cp {remote_path} {local_path} --no-sign-request\", shell=True, check=True)\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = torch.jit.load(local_path).to(device).eval()\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict[str, Any]:  # __call__ now takes a Request object\n",
    "        batch = json.loads(await request.json()) # we will need to parse the JSON body of the request\n",
    "        return await self.predict(batch)\n",
    "    \n",
    "    # same code as MNISTClassifier.predict\n",
    "    async def predict(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033619e",
   "metadata": {},
   "source": [
    "We have now defined our Ray Serve deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f98a83cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deployment(name=OnlineMNISTClassifier,version=None,route_prefix=/OnlineMNISTClassifier)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OnlineMNISTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf85ff1",
   "metadata": {},
   "source": [
    "We can now build an Application using `OnlineMNISTClassifier` deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df46ddd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.serve.deployment.Application at 0x74327f1a4f90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = f\"{storage_folder}/model.pt\" # Use your local path\n",
    "mnist_app = OnlineMNISTClassifier.bind(remote_path=\"s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt\", local_path=model_path, device=\"cpu\")\n",
    "mnist_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e8ac4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note:** `.bind` is a method that takes in the arguments to pass to the Deployment constructor.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e70529",
   "metadata": {},
   "source": [
    "We can then run the application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e96056cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-11 03:09:27,125\tINFO worker.py:1596 -- Connecting to existing Ray cluster at address: 100.92.97.41:6379...\n",
      "2026-02-11 03:09:27,133\tINFO worker.py:1772 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-hxu6nh6hwtp5aam8r7sg1q2zbh.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2026-02-11 03:09:27,135\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_32e7c46329bddc3065de29c0e4db3de3a2177293.zip' (0.13MiB) to Ray cluster...\n",
      "2026-02-11 03:09:27,137\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_32e7c46329bddc3065de29c0e4db3de3a2177293.zip'.\n",
      "\u001b[36m(ProxyActor pid=6193)\u001b[0m INFO 2026-02-11 03:09:31,119 proxy 100.92.97.41 proxy.py:1225 - Proxy starting on node 0861a80cf2e4feb4f682e60a590a1e3e628fbec39420541f9b4d767d (HTTP port: 8000).\n",
      "2026-02-11 03:09:31,162\tINFO handle.py:126 -- Created DeploymentHandle 'ygdrmxrb' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:31,162\tINFO handle.py:126 -- Created DeploymentHandle '8qxmmafc' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "\u001b[36m(ServeController pid=6134)\u001b[0m INFO 2026-02-11 03:09:31,255 controller 6134 deployment_state.py:1598 - Deploying new version of Deployment(name='OnlineMNISTClassifier', app='mnist_classifier') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=6134)\u001b[0m INFO 2026-02-11 03:09:31,358 controller 6134 deployment_state.py:1844 - Adding 1 replica to Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 512.0 KiB/42.7 MiB (1.1 MiB/s) with 1 file(s) remaining  \u001b[0m Completed 256.0 KiB/42.7 MiB (608.9 KiB/s) with 1 file(s) remaining\n",
      "Completed 4.8 MiB/42.7 MiB (8.7 MiB/s) with 1 file(s) remaining    \u001b[0m Completed 768.0 KiB/42.7 MiB (1.7 MiB/s) with 1 file(s) remaining  \n",
      "Completed 21.0 MiB/42.7 MiB (32.1 MiB/s) with 1 file(s) remaining  \u001b[0m Completed 5.0 MiB/42.7 MiB (9.0 MiB/s) with 1 file(s) remaining    \n",
      "Completed 36.0 MiB/42.7 MiB (47.6 MiB/s) with 1 file(s) remaining  \u001b[0m Completed 21.2 MiB/42.7 MiB (32.2 MiB/s) with 1 file(s) remaining  \n",
      "Completed 40.8 MiB/42.7 MiB (48.5 MiB/s) with 1 file(s) remaining  \u001b[0m Completed 36.2 MiB/42.7 MiB (47.2 MiB/s) with 1 file(s) remaining  \n",
      "Completed 42.5 MiB/42.7 MiB (46.1 MiB/s) with 1 file(s) remaining  \u001b[0m Completed 41.0 MiB/42.7 MiB (47.3 MiB/s) with 1 file(s) remaining  \n",
      "\u001b[36m(ServeReplica:mnist_classifier:OnlineMNISTClassifier pid=6269)\u001b[0m Completed 42.7 MiB/42.7 MiB (44.2 MiB/s) with 1 file(s) remaining  \n",
      "\u001b[36m(ServeReplica:mnist_classifier:OnlineMNISTClassifier pid=6269)\u001b[0m download: s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt to ../../../../../../mnt/cluster_storage/model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-11 03:09:39,191\tINFO handle.py:126 -- Created DeploymentHandle 'uw9cjons' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,192\tINFO api.py:574 -- Deployed app 'mnist_classifier' successfully.\n",
      "2026-02-11 03:09:39,197\tINFO handle.py:126 -- Created DeploymentHandle 'uosthhl3' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,197\tINFO handle.py:126 -- Created DeploymentHandle 'ldxq1xrc' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,198\tINFO handle.py:126 -- Created DeploymentHandle '48j7ri79' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,199\tINFO handle.py:126 -- Created DeploymentHandle 'u9gzxqvh' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,200\tINFO handle.py:126 -- Created DeploymentHandle '7wsgxnxf' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,201\tINFO handle.py:126 -- Created DeploymentHandle 'z0gxoh04' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,204\tINFO pow_2_scheduler.py:260 -- Got updated replicas for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier'): {'cn9jccvq'}.\n",
      "2026-02-11 03:09:39,204\tINFO handle.py:126 -- Created DeploymentHandle 'g1io8amd' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,205\tINFO handle.py:126 -- Created DeploymentHandle 'ctyuijpq' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,206\tINFO handle.py:126 -- Created DeploymentHandle '62jmck5d' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,206\tINFO handle.py:126 -- Created DeploymentHandle 's6dmjjji' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n",
      "2026-02-11 03:09:39,207\tINFO handle.py:126 -- Created DeploymentHandle 'k2bsu7xh' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeploymentHandle(deployment='OnlineMNISTClassifier')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_app_handle = serve.run(mnist_app, name='mnist_classifier', blocking=False)\n",
    "mnist_app_handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a0cdb-822a-4439-aeab-9916dd8d059c",
   "metadata": {},
   "source": [
    "We can test it as an HTTP endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0a80e9-c26f-48d2-8985-ef4eab4dc580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.random.rand(2, 1, 28, 28).tolist()\n",
    "json_request = json.dumps({\"image\": images})\n",
    "response = requests.post(\"http://localhost:8000/\", json=json_request)\n",
    "response.json()[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2cb01",
   "metadata": {},
   "source": [
    "We can also test it as a gRPC endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342928ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:mnist_classifier:OnlineMNISTClassifier pid=6269)\u001b[0m INFO 2026-02-11 03:09:39,580 mnist_classifier_OnlineMNISTClassifier cn9jccvq 9d9928b7-2978-4d83-9848-d131c96605a2 / replica.py:408 - __CALL__ OK 155.3ms\n",
      "\u001b[36m(ServeReplica:mnist_classifier:OnlineMNISTClassifier pid=6269)\u001b[0m INFO 2026-02-11 03:09:39,855 mnist_classifier_OnlineMNISTClassifier cn9jccvq 81101a26-2962-475a-8da9-3251e03aecf9 replica.py:408 - PREDICT OK 83.2ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-11 03:09:39,759\tINFO handle.py:126 -- Created DeploymentHandle 'vbezaic2' for Deployment(name='OnlineMNISTClassifier', app='mnist_classifier').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 1, 6, 6, 1, 1, 6, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = {\"image\": np.random.rand(10, 1, 28, 28)}\n",
    "response = await mnist_app_handle.predict.remote(batch)\n",
    "response[\"predicted_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da966d55",
   "metadata": {},
   "source": [
    "## 4. Development workflow\n",
    "\n",
    "1. Define application in a `main.py` file\n",
    "2. Deploy the application with `serve run`\n",
    "3. Optionally specify configuration in a `config.yaml` file\n",
    "    - you can use `serve build` to scaffold a basic config.yaml file\n",
    "    - useful if you want to decouple the deployment configuration from the code\n",
    "4. After making a change \n",
    "    - you can re-run the application with `serve run`\n",
    "    - Note there is experimental support for hot-reloading of changes to the application (using `serve run --reload`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef75ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cd: can't cd to intro/\n"
     ]
    }
   ],
   "source": [
    "# run the app with default config\n",
    "!cd intro/ && serve run main:mnist_app --non-blocking --name app1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ca83adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cd: can't cd to intro/\n"
     ]
    }
   ],
   "source": [
    "# build and optionally customize config\n",
    "!cd intro/ && serve build -o config.yaml main:mnist_app "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a20b2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cd: can't cd to intro/\n"
     ]
    }
   ],
   "source": [
    "# update the running app\n",
    "!cd intro/ && serve run config.yaml --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba25d08",
   "metadata": {},
   "source": [
    "In case you want to **parameterize the application building**, use an \"application builder\" pattern - i.e. set the import path to point to a callable that will return an application.\n",
    "\n",
    "To view an example, see `app_builder.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89696b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cd: can't cd to intro/\n"
     ]
    }
   ],
   "source": [
    "!cd intro/ && serve run app_builder:build_app --non-blocking --name app1 device=cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6a9fc",
   "metadata": {},
   "source": [
    "For more details on the recommended development workflow, read the [docs here](https://docs.ray.io/en/latest/serve/advanced-guides/dev-workflow.html#development-workflow)\n",
    "\n",
    "\n",
    "For unit testing and debugging, Ray Serve provides a local testing mode. For more details, see the [docs here](https://docs.ray.io/en/latest/serve/advanced-guides/dev-workflow.html#local-testing-mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84e18b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm {storage_folder}/model.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
