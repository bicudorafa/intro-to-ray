{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray Data: Ray Data + Structured Data\n",
    "¬© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª **Launch Locally**: You can run this notebook locally, but performance will be reduced.\n",
    "\n",
    "üöÄ **Launch on Cloud**: A Ray Cluster (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will provide an overview of Ray Data and how to use it to load, and transform data in a distributed manner.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li><b>Part 0:</b> What is Ray Data?</a></li>\n",
    "    <li><b>Part 1:</b> How to use Ray Data?</a></li>\n",
    "    <li><b>Part 2:</b> Loading Data</a></li>\n",
    "    <li><b>Part 3:</b> Transforming Data</a></li>\n",
    "    <li><b>Part 4:</b> Writing Data</a></li>\n",
    "    <li><b>Part 5:</b> Data Operations: Shuffling, Grouping and Aggregation</a></li>\n",
    "    <li><b>Part 6:</b> When to use Ray Data</a></li>\n",
    "    <li><b>Part 7:</b> Ray Data in Production</a></li>\n",
    "    <li><b>Part 8:</b> Upcoming Features in Ray Data</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. What is Ray Data?\n",
    "\n",
    "Ray Data is a distributed data processing library that provides a Python API for parallel data processing. \n",
    "\n",
    "It is built on top of Ray, a fast and simple framework for building and running distributed applications. Ray Data is designed to be easy to use, scalable, and fault-tolerant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How to Use Ray Data?\n",
    "\n",
    "You typically should use the Ray Data API in this way:\n",
    "\n",
    "1. **Create a Ray Dataset** from external storage or in-memory data.\n",
    "2. **Apply transformations** to the data.\n",
    "3. **Write the outputs** to external storage or **feed the outputs** to training workers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "\n",
    "Our Dataset is the New York City Taxi & Limousine Commission's Trip Record Data\n",
    "\n",
    "**Dataset features**\n",
    "\n",
    "| Column | Description | \n",
    "| ------ | ----------- |\n",
    "| `trip_distance` | Float representing trip distance in miles. |\n",
    "| `passenger_count` | The number of passengers |\n",
    "| `PULocationID` | TLC Taxi Zone in which the taximeter was engaged | \n",
    "| `DOLocationID` | TLC Taxi Zone in which the taximeter was disengaged | \n",
    "| `payment_type` | A numeric code signifying how the passenger paid for the trip. |\n",
    "| `tolls_amount` | Total amount of all tolls paid in trip. | \n",
    "| `tip_amount` | Tip amount ‚Äì This field is automatically populated for credit card tips. Cash tips are not included. | \n",
    "| `total_amount` | The total amount charged to passengers. Does not include cash tips. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\n",
    "    \"trip_distance\",\n",
    "    \"passenger_count\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"payment_type\",\n",
    "    \"tolls_amount\",\n",
    "    \"tip_amount\",\n",
    "    \"total_amount\",\n",
    "]\n",
    "\n",
    "DATA_PATH = \"s3://anyscale-public-materials/nyc-taxi-cab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data for a single month. It takes up to 2 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.48</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>231</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.63</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.10</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.80</td>\n",
       "      <td>1</td>\n",
       "      <td>234</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.70</td>\n",
       "      <td>1</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_distance  passenger_count  PULocationID  DOLocationID  payment_type  \\\n",
       "0           1.48                1           148           231             2   \n",
       "1           1.63                1           132           132             1   \n",
       "2           1.10                2            79           107             1   \n",
       "3           2.80                1           234           141             1   \n",
       "4           1.70                1           237           140             1   \n",
       "\n",
       "   tolls_amount  tip_amount  total_amount  \n",
       "0           0.0         0.0           8.7  \n",
       "1           0.0         2.0          11.5  \n",
       "2           0.0         1.0           8.1  \n",
       "3           0.0         1.5          12.2  \n",
       "4           0.0         1.0           8.9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\n",
    "    f\"{DATA_PATH}/yellow_tripdata_2011-05.parquet\",\n",
    "    columns=COLUMNS,\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how much memory the dataset is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory usage: 949.39 MB\n"
     ]
    }
   ],
   "source": [
    "total_memory_usage = df.memory_usage(deep=True).sum().sum() / 1024**2 # result will be in MB\n",
    "print(f\"Total memory usage: {total_memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many files there are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 156\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 ls s3://anyscale-public-materials/nyc-taxi-cab/ --human-readable | wc -l\n",
    "# 156 files (it won't work without installing the aws cli)\n",
    "print(f\"Number of files: {156}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not making use of all the columns and are already consuming ~1GB of data per file -> will quickly become a problem if you want to scale to entire dataset (~155 files) if we are running on a small node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instead make use of a distributed data preprocessing library like Ray Data to load the full dataset in a distributed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5c2585f76c490996295363a5f7a55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Metadata Fetch Progress 0:   0%|          | 0.00/25.0 [00:00<?, ? task/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:11:53,306\tINFO worker.py:1879 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481ed969d98f49119e932d986be62bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parquet Files Sample 0:   0%|          | 0.00/2.00 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ray.data.read_parquet(\n",
    "    DATA_PATH,\n",
    "    columns=COLUMNS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Ray data equivalents for common pandas functions like `read_csv`, `read_parquet`, `read_json`, etc.\n",
    "\n",
    "Refer to the [Input/Output docs](https://docs.ray.io/en/latest/data/api/input_output.html) for a comprehensive list of read functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Let's view our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abfb79cfa21402e9b2a456f6bb7f180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset(\n",
       "   num_rows=1340417181,\n",
       "   schema={\n",
       "      trip_distance: double,\n",
       "      passenger_count: int64,\n",
       "      PULocationID: int64,\n",
       "      DOLocationID: int64,\n",
       "      payment_type: int64,\n",
       "      tolls_amount: double,\n",
       "      tip_amount: double,\n",
       "      total_amount: double\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Data by default adopts **lazy execution** this means that the data is not loaded into memory until it is needed. Instead only a small part of the dataset is loaded into memory to infer the schema. (a.k.a execute \"ds\" is equivalente to pd.DataFrame.head(), or PySpark Limite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dataset specifies a sequence of transformations that will be applied to the data. \n",
    "\n",
    "The data itself will be organized into blocks, where each block is a collection of rows.\n",
    "\n",
    "The following figure visualizes a tabular dataset with three blocks, each block holding 1000 rows each:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/dataset-arch.svg' width=50%/>\n",
    "\n",
    "Since a Dataset is just a list of Ray object references, it can be freely passed between Ray tasks, actors, and libraries like any other object reference. This flexibility is a unique characteristic of Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transforming Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple function to generate features from the data. Here is how we would do so using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_total_amount(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"adjusted_total_amount\"] = df[\"total_amount\"] - df[\"tip_amount\"]\n",
    "    return df\n",
    "\n",
    "df = adjust_total_amount(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the same function and apply it to the Ray dataset using `map_batches`. \n",
    "\n",
    "`map_batches` will batch each block of the dataset and apply the function to each batch in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_adjusted = ds.map_batches(adjust_total_amount, batch_format=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b> \n",
    "\n",
    "The default `batch_format` in Ray Data is `numpy`, which means that the data is returned as a numpy array. For optimal performance, it is recommended to **avoid converting the data to pandas dataframes unless necessary**.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another transformation, for the sake of this example, we will add a simple transformation to calculate the tip percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tip_percentage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"tip_percentage\"] = df[\"tip_amount\"] / df[\"total_amount\"]\n",
    "    return df\n",
    "\n",
    "df = compute_tip_percentage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would apply it again using `map_batches`. Note that we can control certain additional parameters such as the batch size to use. (how does batch_size relates to the number of rows by each ObjectRef? Does it make some kind of repartition?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how does batch_size relates to the number of rows by each ObjectRef? Does it make some kind of repartition?\n",
    "ds_tip = ds_adjusted.map_batches(compute_tip_percentage, batch_format=\"pandas\", batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution mode\n",
    "\n",
    "Most transformations are **lazy** in Ray Data - i.e. they don't execute until you either:\n",
    "- **write a dataset to storage**\n",
    "- explicitly **materialize** the data\n",
    "- **iterate over the dataset** (usually when feeding data to model training).\n",
    "\n",
    "To explicitly *materialize* a very small subset of the data, you can use the `take_batch` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:31:11,900\tINFO logging.py:290 -- Registered dataset logger for dataset dataset_10_0\n",
      "2025-12-17 00:31:11,906\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_10_0. Full logs are in /tmp/ray/session_2025-12-17_00-11-49_070128_44566/logs/ray-data\n",
      "2025-12-17 00:31:11,907\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_10_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=20]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77db30947905472ebfe8e84bc91419af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c920d24005474aa6814d67e1b26c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadParquet->SplitBlocks(25) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56459359181648adb9bacdbb50ee9dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=20 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:31:33,744\tINFO streaming_executor.py:220 -- ‚úîÔ∏è  Dataset dataset_10_0 execution finished in 21.84 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trip_distance': array([ 1.48,  1.63,  1.1 ,  2.8 ,  1.7 ,  0.5 ,  1.4 ,  0.8 ,  2.1 ,\n",
       "         1.3 ,  2.6 ,  0.8 ,  1.9 ,  0.7 ,  1.3 ,  1.6 ,  2.1 ,  0.8 ,\n",
       "        19.2 ,  0.  ]),\n",
       " 'passenger_count': array([1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2]),\n",
       " 'PULocationID': array([148, 132,  79, 234, 237, 237, 161, 186,  90, 229,  48, 234, 170,\n",
       "         79, 107, 114, 231, 234, 132, 162]),\n",
       " 'DOLocationID': array([231, 132, 107, 141, 140, 237, 234,  90, 161,  48, 107, 170,  79,\n",
       "        107, 114, 231, 234, 137, 263, 164]),\n",
       " 'payment_type': array([2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2]),\n",
       " 'tolls_amount': array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "        0. , 0. , 0. , 0. , 0. , 4.8, 0. ]),\n",
       " 'tip_amount': array([ 0.  ,  2.  ,  1.  ,  1.5 ,  1.  ,  1.  ,  0.  ,  1.02,  1.82,\n",
       "         0.  ,  1.75,  0.  ,  2.73,  1.7 ,  0.  ,  1.  ,  0.  ,  0.  ,\n",
       "        10.06,  0.  ]),\n",
       " 'total_amount': array([ 8.7 , 11.5 ,  8.1 , 12.2 ,  8.9 ,  5.7 ,  7.1 ,  6.12, 10.92,\n",
       "         7.9 , 12.45,  5.9 , 11.83,  8.  ,  9.1 , 11.3 ,  9.5 ,  5.5 ,\n",
       "        60.36,  8.3 ])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view a batch of the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:31:33,764\tINFO logging.py:290 -- Registered dataset logger for dataset dataset_11_0\n",
      "2025-12-17 00:31:33,772\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_11_0. Full logs are in /tmp/ray/session_2025-12-17_00-11-49_070128_44566/logs/ray-data\n",
      "2025-12-17 00:31:33,772\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_11_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(adjust_total_amount)->MapBatches(compute_tip_percentage)] -> LimitOperator[limit=20]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c33294625fa4119959625d9a3ed52c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146790194cba415a9ad9955b4513f4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadParquet 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aa7ded259248cc995906b8c5c7946b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(adjust_total_amount)->MapBatches(compute_tip_percentage) 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa73cf942304163870ae8111f37a6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=20 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:31:50,633\tINFO streaming_executor.py:220 -- ‚úîÔ∏è  Dataset dataset_11_0 execution finished in 16.86 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trip_distance': array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 8. , 1.6, 2.5, 3.9, 0.6, 0.9,\n",
       "        3.9, 0.7, 1.9, 3.3, 5.3, 1. , 4.3]),\n",
       " 'passenger_count': array([4, 4, 4, 5, 5, 1, 1, 1, 1, 4, 2, 2, 4, 1, 2, 2, 2, 2, 1, 1]),\n",
       " 'PULocationID': array([145, 264, 264, 146, 146, 146, 146, 138, 170, 237, 170,  90,  90,\n",
       "         90, 113,  79, 170, 142, 114, 249]),\n",
       " 'DOLocationID': array([145, 264, 264, 146, 146, 146, 146, 256, 237, 170, 239,  90, 186,\n",
       "        238,  79, 170, 142, 112, 249, 143]),\n",
       " 'payment_type': array([1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2]),\n",
       " 'tolls_amount': array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "        0. , 0. , 0. , 0. , 4.8, 0. , 0. ]),\n",
       " 'tip_amount': array([0.28, 0.24, 1.11, 0.  , 0.11, 0.  , 0.  , 0.  , 0.  , 0.  , 2.38,\n",
       "        0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ]),\n",
       " 'total_amount': array([ 4.18,  6.94,  5.01,  3.9 ,  3.61,  3.5 ,  3.5 , 21.1 , 10.3 ,\n",
       "         9.1 , 18.28,  5.1 ,  7.5 , 16.3 ,  4.6 ,  8.4 , 11.8 , 25.4 ,\n",
       "         6.3 , 14.7 ]),\n",
       " 'adjusted_total_amount': array([ 3.9,  6.7,  3.9,  3.9,  3.5,  3.5,  3.5, 21.1, 10.3,  9.1, 15.9,\n",
       "         5.1,  7.5, 16.3,  4.6,  7.4, 11.8, 25.4,  6.3, 14.7]),\n",
       " 'tip_percentage': array([0.06698565, 0.03458213, 0.22155689, 0.        , 0.03047091,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.13019694, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.11904762, 0.        , 0.        , 0.        , 0.        ])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tip.take_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the adjusted data. Here is how we would do it with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "working_dir = os.getcwd()\n",
    "# storage_folder = '/mnt/cluster_storage' # Modify this path to your local folder if it runs on your local environment\n",
    "storage_folder = working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f\"{storage_folder}/adjusted_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the file we just wrote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 rbrosa  staff   140M Dec 16 00:51 /Users/rbrosa/Documents/github_personal/intro-to-ray/notebooks/ray_data/adjusted_data.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {storage_folder}/adjusted_data.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we would do so with Ray Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:32:06,032\tINFO logging.py:290 -- Registered dataset logger for dataset dataset_14_0\n",
      "2025-12-17 00:32:06,037\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_14_0. Full logs are in /tmp/ray/session_2025-12-17_00-11-49_070128_44566/logs/ray-data\n",
      "2025-12-17 00:32:06,038\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_14_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet->MapBatches(adjust_total_amount)] -> LimitOperator[limit=15554868] -> TaskPoolMapOperator[Write]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957c0b6617344f9fb77a5626d4c60116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5804fb496a254016b549cbcfe920e888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadParquet->MapBatches(adjust_total_amount) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34c8b28ce6141b5bfee58eef9ee2b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=15554868 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01596fa8b2fb4c58b9c76290e0ac2342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Write 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:32:53,631\tINFO streaming_executor.py:220 -- ‚úîÔ∏è  Dataset dataset_14_0 execution finished in 47.59 seconds\n",
      "2025-12-17 00:32:53,666\tINFO dataset.py:4537 -- Data sink Parquet finished. 15554868 rows and 1.0GB data written.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf {storage_folder}/adjusted_data_ray/ # let's remove the directory if it exists\n",
    "ds_limited = ds_adjusted.limit(df.shape[0]) # we limit to avoid writing too much data\n",
    "ds_limited.write_parquet(f\"{storage_folder}/adjusted_data_ray/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Ray data equivalents for common pandas functions like `write_parquet` for `to_parquet`, `write_csv` for `to_csv`, etc.\n",
    "\n",
    "See the [Input/Output docs](https://docs.ray.io/en/latest/data/api/input_output.html) for a comprehensive list of write functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the files in the directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 245488\n",
      "-rw-r--r--@ 1 rbrosa  staff    14M Dec 16 00:52 7_000000_000000.parquet\n",
      "-rw-r--r--@ 1 rbrosa  staff    15M Dec 16 00:52 7_000001_000000.parquet\n",
      "-rw-r--r--@ 1 rbrosa  staff    14M Dec 16 00:52 7_000002_000000.parquet\n",
      "-rw-r--r--@ 1 rbrosa  staff    14M Dec 16 00:52 7_000003_000000.parquet\n",
      "-rw-r--r--@ 1 rbrosa  staff    15M Dec 16 00:52 7_000004_000000.parquet\n",
      "-rw-r--r--@ 1 rbrosa  staff    14M Dec 16 00:52 7_000005_000000.parquet\n",
      "-rw-r--r--@ 1 rbrosa  staff    15M Dec 16 00:52 7_000006_000000.parquet\n",
      "-rw-r--r--@ 1 rbrosa  staff   2.9M Dec 16 00:52 7_000007_000000.parquet\n",
      "-rw-r--r--@ 1 rbrosa  staff    14M Dec 16 00:52 7_000008_000000.parquet\n",
      "-rw-r--r--@ 1 rbrosa  staff   1.7M Dec 16 00:52 7_000009_000000.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {storage_folder}/adjusted_data_ray/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have **multiple files** in the directory. This is because Ray Data writes data in a **distributed manner**. \n",
    "\n",
    "**Each task writes its own file**, and the number of files is proportional to the number of CPUs in the cluster.\n",
    "\n",
    "**Ray Data uses Ray tasks** to process data.\n",
    "\n",
    "When reading from a file-based datasource (e.g., S3, GCS). Each read task reads its assigned files and produces an output block which in turn is consumed by the next task in the pipeline.\n",
    "    \n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/dataset-read-cropped-v2.svg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b> \n",
    "\n",
    "We passed `/mnt/cluster_storage/` as the path to write the data. This is a path on the Ray cluster's shared storage. If instead you use a path that is only local to one of the nodes in a multi-node cluster, you will see errors like `FileNotFoundError: [Errno 2] No such file or directory: '/path/to/file'`.\n",
    "\n",
    "This is because Ray Data is designed to work with distributed storage systems like S3, HDFS, etc. If you want to write to local storage, you can add a special prefix `local://` to the path. For example, `local:///path/to/file`. However to do so you will need to ensure that Ray is enabled to schedule and run tasks on the head node of the cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Operations: Shuffling, Grouping and Aggregation\n",
    "\n",
    "Let's look at some more involved transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling data \n",
    "\n",
    "There are different options to shuffle data in Ray Data of varying degrees of randomness and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File based shuffle on read\n",
    "\n",
    "To randomly shuffle the ordering of input files before reading, use the shuffle=\"files\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13adb832856408bb67d4c246fe08c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Metadata Fetch Progress 0:   0%|          | 0.00/25.0 [00:00<?, ? task/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f1679c66bd446faf416882f4735dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parquet Files Sample 0:   0%|          | 0.00/2.00 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_file_shuffled = ray.data.read_parquet(DATA_PATH, columns=COLUMNS, shuffle=\"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b0181856ce48ab92590c7cfaf1d77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset(\n",
       "   num_rows=1340417181,\n",
       "   schema={\n",
       "      trip_distance: double,\n",
       "      passenger_count: int64,\n",
       "      PULocationID: int64,\n",
       "      DOLocationID: int64,\n",
       "      payment_type: int64,\n",
       "      tolls_amount: double,\n",
       "      tip_amount: double,\n",
       "      total_amount: double\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_file_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling block order\n",
    "This option randomizes the order of blocks in a dataset.\n",
    "\n",
    "Applying this operation alone doesn‚Äôt involve heavy computation and communication. However, it requires Ray Data to materialize all blocks before applying the operation.\n",
    "\n",
    "Let's read the data and shuffle the block order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60199c6011094f99befa7620483fe683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parquet Files Sample 0:   0%|          | 0.00/1.00 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = (\n",
    "    ray.data.read_parquet(\n",
    "        \"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_2011-05.parquet\",\n",
    "        columns=COLUMNS,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform block order shuffling, use `randomize_block_order`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:26:54,300\tINFO logging.py:290 -- Registered dataset logger for dataset dataset_7_0\n",
      "2025-12-17 00:26:54,310\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_7_0. Full logs are in /tmp/ray/session_2025-12-17_00-11-49_070128_44566/logs/ray-data\n",
      "2025-12-17 00:26:54,317\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_7_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> AllToAllOperator[RandomizeBlockOrder]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302b9a22c52e40bf8bcfb4fd8054aa16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a96a2366604dc4b6ebc8e62d126f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadParquet->SplitBlocks(25) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824f548fbe0645d5acf35bd0cf8c8f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- RandomizeBlockOrder 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:27:19,725\tINFO streaming_executor.py:220 -- ‚úîÔ∏è  Dataset dataset_7_0 execution finished in 25.42 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.70</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.10</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>162</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.90</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.07</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>6.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15554863</th>\n",
       "      <td>2.60</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>265</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>11.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15554864</th>\n",
       "      <td>1.41</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15554865</th>\n",
       "      <td>4.67</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>14.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15554866</th>\n",
       "      <td>0.96</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15554867</th>\n",
       "      <td>13.72</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15554868 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          trip_distance  passenger_count  PULocationID  DOLocationID  \\\n",
       "0                  1.70                1           230            90   \n",
       "1                  3.10                1            90           162   \n",
       "2                  2.90                1           230           141   \n",
       "3                 12.07                1           132            49   \n",
       "4                  0.00                1            79           114   \n",
       "...                 ...              ...           ...           ...   \n",
       "15554863           2.60                1           150           265   \n",
       "15554864           1.41                1           265           265   \n",
       "15554865           4.67                1           265           265   \n",
       "15554866           0.96                1           265           265   \n",
       "15554867          13.72                1            65            79   \n",
       "\n",
       "          payment_type  tolls_amount  tip_amount  total_amount  \n",
       "0                    2           0.0        0.00          6.60  \n",
       "1                    2           0.0        0.00         10.20  \n",
       "2                    2           0.0        0.00          9.40  \n",
       "3                    2           0.0        0.00         32.60  \n",
       "4                    1           0.0        1.15          6.95  \n",
       "...                ...           ...         ...           ...  \n",
       "15554863             1           0.0        2.00         11.10  \n",
       "15554864             2           0.0        0.00          7.90  \n",
       "15554865             1           0.0        0.50         14.40  \n",
       "15554866             2           0.0        0.00          5.90  \n",
       "15554867             2           0.0        0.00         32.30  \n",
       "\n",
       "[15554868 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_block_based_shuffle = ds.randomize_block_order()\n",
    "ds_block_based_shuffle.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle all rows globally\n",
    "To randomly shuffle all rows globally, call `random_shuffle()`. This is the slowest option for shuffle, and requires transferring data across network between workers. This option achieves the best randomness among all options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_row_based_shuffle = ds.random_shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_row_based_shuffle.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom batching using `groupby` and aggregations\n",
    "\n",
    "In case you want to generate batches according to a specific key, you can use `groupby` to group the data by the key and then use `map_groups` to apply the transformation.\n",
    "\n",
    "For instance, let's compute the average trip distance per passenger count. Here is how we would do it with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "payment_type\n",
       "1    3.192318\n",
       "2    2.592786\n",
       "Name: trip_distance, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"payment_type\")[\"trip_distance\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we would do the same operation with Ray Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:28:50,717\tINFO logging.py:290 -- Registered dataset logger for dataset dataset_9_0\n",
      "2025-12-17 00:28:50,726\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_9_0. Full logs are in /tmp/ray/session_2025-12-17_00-11-49_070128_44566/logs/ray-data\n",
      "2025-12-17 00:28:50,726\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_9_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> AllToAllOperator[Repartition] -> AllToAllOperator[Aggregate]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ea93158ab24f70949f96aa7647b789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eedb60fc30b4fc08934f86ff3b554e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadParquet->SplitBlocks(200) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554f1c9ffeab4dc69eee0bf0559dea49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Repartition 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd615abafaa481f8a6ce94a4e2f186e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split Repartition 3:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66e14d19db24950a670afb51a75bfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Aggregate 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3d661eaf974fb491b71786fd594d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 5:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ce0ea1b5ef4f04b9f248c58da9ed15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 6:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3f087fc440463588d87d983c948655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 7:   0%|          | 0.00/1.00 [00:00<?, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 00:29:26,034\tINFO streaming_executor.py:220 -- ‚úîÔ∏è  Dataset dataset_9_0 execution finished in 35.31 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_type</th>\n",
       "      <th>mean(trip_distance)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.192318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.592786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_type  mean(trip_distance)\n",
       "0             1             3.192318\n",
       "1             2             2.592786"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cpus = 8\n",
    "ds.repartition(num_cpus).groupby(\"payment_type\").mean(\"trip_distance\").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the main aggregation functions available in Ray Data:\n",
    "- count\n",
    "- max\n",
    "- mean\n",
    "- min\n",
    "- sum\n",
    "- std\n",
    "\n",
    "See [relevant docs page here](https://docs.ray.io/en/latest/data/api/grouped_data.html#computations-or-descriptive-stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Note:</b> This is an area of active development in Ray Data. The current implementation of groupby is not as optimized as it could be. We are working on improving the performance of `groupby` and `map_groups` operations.\n",
    "\n",
    "For more details, the current implementation makes use of a sort operation which instead can be done using a hash-based implementation. Additionally, we had to repartition the data to maximize parallelism - in the future Ray Data should be able to dynamically repartition the data to maximize parallelism.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. When to use Ray Data\n",
    "\n",
    "Ray Data is especially performant when needing to:\n",
    "- run data processing in a **streaming fashion** \n",
    "- run across a **large dataset**\n",
    "- run inside a **heterogeneous cluster of CPUs and GPUs**.\n",
    "\n",
    "Here is one use case for Batch Inference with Ray Data over a large dataset:\n",
    "\n",
    "<img src='https://docs.ray.io/en/releases-2.6.1/_images/stream-example.png' width=60%/>\n",
    "\n",
    "\n",
    "Ray Data also integrates seamlessly with Ray Train, making it an optimal choice for **data preprocessing in machine learning training pipelines**. Especially when you need to:\n",
    "- **Independently scale out data loading and transformation** from model training.\n",
    "- **Enable fault tolerance** for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ray Data in Production\n",
    "\n",
    "1. Runway AI is using Ray Data to scale its ML workloads. See [this interview with Runway AI](https://siliconangle.com/2024/10/02/runway-transforming-ai-driven-filmmaking-innovative-tools-techniques-raysummit/) to learn more.\n",
    "2. Netflix is using Ray Data for multi-modal batch inference pipelines. See [this talk at the Ray Summit 2024](https://raysummit.anyscale.com/flow/anyscale/raysummit2024/landing/page/sessioncatalog/session/1722028596844001bCg0) to learn more.\n",
    "3. Spotify uses Ray Data for large-scale data processing. See [this talk at the Ray Summit 2023](https://www.anyscale.com/blog/how-spotify-built-a-robust-ray-platform-with-a-frictionless-developer) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Upcoming Features in Ray Data\n",
    "\n",
    "Here are some relevant upcoming features in Ray Data:\n",
    "\n",
    "For structured data:\n",
    "- improved `groupby` and `map_groups` performance\n",
    "- using parquet metadata for computing statistics like `count`\n",
    "- enabling predicate pushdown for parquet files when calling `filter`\n",
    "- supporting `join` and `merge` operations\n",
    "- optimizing performance of the `Preprocessor` API for distributed feature engineering\n",
    "- running spark on Ray more seamlessly\n",
    "\n",
    "\n",
    "For all data types:\n",
    "- data checkpointing for fault tolerance\n",
    "- optimizing data connectors\n",
    "- concurrent execution of multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm {storage_folder}/adjusted_data.parquet\n",
    "!rm -rf {storage_folder}/adjusted_data_ray/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
