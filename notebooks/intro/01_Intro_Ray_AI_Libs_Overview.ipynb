{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to the Ray AI Libraries: An example of using Ray data, Ray Train, Ray Tune, Ray Serve to implement a XGBoost regression model\n",
    "\n",
    "¬© 2025, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª **Launch Locally**: You can run this notebook locally, but performance will be reduced.\n",
    "\n",
    "üöÄ **Launch on Cloud**: A Ray Cluster with 4 GPUs (Click [here](http://console.anyscale.com/register) to easily start a Ray cluster on Anyscale) is recommended to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a quick end-to-end example to get a sense of what the Ray AI Libraries can do.\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Here is the roadmap for this notebook:</b>\n",
    "<ul>\n",
    "    <li>Overview of the Ray AI Libraries</li>\n",
    "    <li>Quick end-to-end example</li>\n",
    "    <ul>\n",
    "      <li>Vanilla XGBoost code</li>\n",
    "      <li>Hyperparameter tuning with Ray Tune</li>\n",
    "      <li>Distributed training with Ray Train</li>\n",
    "      <li>Serving an ensemble model with Ray Serve</li>\n",
    "      <li>Batch inference with Ray Data</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional): If you get an XGBoostError at import, you might have to `brew install libomp` before importing xgboost again\n",
    "# !brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import fastapi\n",
    "import pandas as pd\n",
    "import requests\n",
    "# macos: If you get an XGBoostError at import, you might have to `brew install libomp` before importing xgboost again\n",
    "import xgboost\n",
    "from pydantic import BaseModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ray\n",
    "import ray.tune\n",
    "import ray.train\n",
    "from ray.train.xgboost import XGBoostTrainer as RayTrainXGBoostTrainer\n",
    "from ray.train import RunConfig\n",
    "import ray.data\n",
    "import ray.serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of the Ray AI Libraries\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_AI_Libraries/Ray+AI+Libraries.png\" width=\"700px\" loading=\"lazy\">\n",
    "\n",
    "Built on top of Ray Core, the Ray AI Libraries inherit all the performance and scalability benefits offered by Core while providing a convenient abstraction layer for machine learning. These Python-first native libraries allow ML practitioners to distribute individual workloads, end-to-end applications, and build custom use cases in a unified framework.\n",
    "\n",
    "The Ray AI Libraries bring together an ever-growing ecosystem of integrations with popular machine learning frameworks to create a common interface for development.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=\"100%\" loading=\"lazy\">|\n",
    "|:-:|\n",
    "|Ray AI Libraries enable end-to-end ML development and provides multiple options for integrating with other tools and libraries from the MLOps ecosystem.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick end-to-end example\n",
    "\n",
    "For this regression task, you will apply a simple [XGBoost](https://xgboost.readthedocs.io/en/stable/) (a gradient boosted trees framework) model to the June 2021 [New York City Taxi & Limousine Commission's Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). \n",
    "\n",
    "The full dataset contains millions of samples of yellow cab rides, and the goal is to predict the tip amount.\n",
    "\n",
    "**Dataset features**\n",
    "* **`passenger_count`**\n",
    "    * Float (whole number) representing number of passengers.\n",
    "* **`trip_distance`** \n",
    "    * Float representing trip distance in miles.\n",
    "* **`fare_amount`**\n",
    "    * Float representing total price including tax, tip, fees, etc.\n",
    "* **`tolls_amount`**\n",
    "    * Float representing the total paid on tolls if any.\n",
    "\n",
    "**Target**\n",
    "* **`trip_amount`**\n",
    "    * Float representing the total paid as tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vanilla XGboost code\n",
    "\n",
    "Let's start with the vanilla XGBoost code to predict the tip amount for a NYC taxi cab data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"passenger_count\", \n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"tolls_amount\",\n",
    "]\n",
    "\n",
    "label_column = \"tip_amount\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to load the data and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    path = \"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_2021-03.parquet\"\n",
    "    df = pd.read_parquet(path, columns=features + [label_column])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features], df[label_column], test_size=0.2, random_state=42\n",
    "    )\n",
    "    dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to run `xgboost.train` given some hyperparameter dictionary `params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# storage_folder = \"/mnt/cluster_storage/\" # Modify this path to your local folder if it runs on your local environment\n",
    "storage_folder = Path.cwd() / \"models\"\n",
    "storage_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-rmse:2.18114\n",
      "[1]\teval-rmse:2.13805\n",
      "[2]\teval-rmse:2.10221\n",
      "[3]\teval-rmse:2.07294\n",
      "[4]\teval-rmse:2.04855\n",
      "[5]\teval-rmse:2.02852\n",
      "[6]\teval-rmse:2.01225\n",
      "[7]\teval-rmse:1.99868\n",
      "[8]\teval-rmse:1.98771\n",
      "[9]\teval-rmse:1.97872\n",
      "OrderedDict([('rmse', [2.18113709207776, 2.138052274494217, 2.1022143627953036, 2.072936825276888, 2.0485457212693987, 2.028522863406997, 2.0122461934067273, 1.99868078532301, 1.9877117047436585, 1.9787180742813582])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval-rmse': 1.9787180742813582}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "model_path = Path(storage_folder) / \"model.ubj\"\n",
    "\n",
    "def my_xgboost_func(params):    \n",
    "    evals_result = {}\n",
    "    dtrain, dtest = load_data()\n",
    "    bst = xgboost.train(\n",
    "        params, \n",
    "        dtrain, \n",
    "        num_boost_round=10, \n",
    "        evals=[(dtest, \"eval\")], \n",
    "        evals_result=evals_result,\n",
    "    )\n",
    "    # Use Path\n",
    "    bst.save_model(model_path)\n",
    "    print(f\"{evals_result['eval']}\")\n",
    "    return {\"eval-rmse\": evals_result[\"eval\"][\"rmse\"][-1]}\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "}\n",
    "my_xgboost_func(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyperparameter tuning with Ray Tune\n",
    "\n",
    "Let's use Ray Tune to run distributed hyperparameter tuning for the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-11-24 23:43:25</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:29.31        </td></tr>\n",
       "<tr><td>Memory:      </td><td>19.1/24.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/10 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">      eta</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  eval-rmse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>my_xgboost_func_71de2_00000</td><td>TERMINATED</td><td>127.0.0.1:36564</td><td style=\"text-align: right;\">0.0287829</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         14.5091</td><td style=\"text-align: right;\">    2.10979</td></tr>\n",
       "<tr><td>my_xgboost_func_71de2_00001</td><td>TERMINATED</td><td>127.0.0.1:36561</td><td style=\"text-align: right;\">0.115652 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         17.5412</td><td style=\"text-align: right;\">    1.96708</td></tr>\n",
       "<tr><td>my_xgboost_func_71de2_00002</td><td>TERMINATED</td><td>127.0.0.1:36571</td><td style=\"text-align: right;\">0.19951  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         15.2727</td><td style=\"text-align: right;\">    1.9408 </td></tr>\n",
       "<tr><td>my_xgboost_func_71de2_00003</td><td>TERMINATED</td><td>127.0.0.1:36569</td><td style=\"text-align: right;\">0.200837 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         15.6643</td><td style=\"text-align: right;\">    1.94069</td></tr>\n",
       "<tr><td>my_xgboost_func_71de2_00004</td><td>TERMINATED</td><td>127.0.0.1:36570</td><td style=\"text-align: right;\">0.0176332</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.2013</td><td style=\"text-align: right;\">    2.15055</td></tr>\n",
       "<tr><td>my_xgboost_func_71de2_00005</td><td>TERMINATED</td><td>127.0.0.1:36573</td><td style=\"text-align: right;\">0.0252701</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.9083</td><td style=\"text-align: right;\">    2.1218 </td></tr>\n",
       "<tr><td>my_xgboost_func_71de2_00006</td><td>TERMINATED</td><td>127.0.0.1:36577</td><td style=\"text-align: right;\">0.132889 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         14.2887</td><td style=\"text-align: right;\">    1.95799</td></tr>\n",
       "<tr><td>my_xgboost_func_71de2_00007</td><td>TERMINATED</td><td>127.0.0.1:36566</td><td style=\"text-align: right;\">0.0371789</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.3904</td><td style=\"text-align: right;\">    2.08404</td></tr>\n",
       "<tr><td>my_xgboost_func_71de2_00008</td><td>TERMINATED</td><td>127.0.0.1:36562</td><td style=\"text-align: right;\">0.0652335</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         12.3552</td><td style=\"text-align: right;\">    2.02175</td></tr>\n",
       "<tr><td>my_xgboost_func_71de2_00009</td><td>TERMINATED</td><td>127.0.0.1:36563</td><td style=\"text-align: right;\">0.280744 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         11.1995</td><td style=\"text-align: right;\">    1.93489</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m [0]\teval-rmse:2.09867\n",
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m [1]\teval-rmse:2.02469\n",
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m [2]\teval-rmse:1.98430\n",
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m [3]\teval-rmse:1.96273\n",
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m [4]\teval-rmse:1.95101\n",
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m [5]\teval-rmse:1.94453\n",
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m [6]\teval-rmse:1.93984\n",
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m [7]\teval-rmse:1.93714\n",
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m [8]\teval-rmse:1.93584\n",
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m [9]\teval-rmse:1.93489\n",
      "\u001b[36m(my_xgboost_func pid=36563)\u001b[0m OrderedDict([('rmse', [2.098670914531913, 2.0246899103885965, 1.9843037115534818, 1.962731811611558, 1.9510121628264807, 1.9445273273346526, 1.9398359707897646, 1.937144672172907, 1.9358370394237943, 1.9348927970478986])])\n",
      "\u001b[36m(my_xgboost_func pid=36562)\u001b[0m [0]\teval-rmse:2.19870\n",
      "\u001b[36m(my_xgboost_func pid=36562)\u001b[0m [1]\teval-rmse:2.16820\n",
      "\u001b[36m(my_xgboost_func pid=36562)\u001b[0m [2]\teval-rmse:2.14112\n",
      "\u001b[36m(my_xgboost_func pid=36562)\u001b[0m [3]\teval-rmse:2.11704\n",
      "\u001b[36m(my_xgboost_func pid=36562)\u001b[0m [4]\teval-rmse:2.09590\n",
      "\u001b[36m(my_xgboost_func pid=36562)\u001b[0m [5]\teval-rmse:2.07713\n",
      "\u001b[36m(my_xgboost_func pid=36562)\u001b[0m [6]\teval-rmse:2.06050\n",
      "\u001b[36m(my_xgboost_func pid=36573)\u001b[0m [0]\teval-rmse:2.21953\n",
      "\u001b[36m(my_xgboost_func pid=36573)\u001b[0m [1]\teval-rmse:2.20660\n",
      "\u001b[36m(my_xgboost_func pid=36562)\u001b[0m [7]\teval-rmse:2.04586\n",
      "\u001b[36m(my_xgboost_func pid=36573)\u001b[0m [2]\teval-rmse:2.19425\n",
      "\u001b[36m(my_xgboost_func pid=36562)\u001b[0m [8]\teval-rmse:2.03297\n",
      "\u001b[36m(my_xgboost_func pid=36562)\u001b[0m [9]\teval-rmse:2.02175\n",
      "\u001b[36m(my_xgboost_func pid=36573)\u001b[0m [3]\teval-rmse:2.18245\n",
      "\u001b[36m(my_xgboost_func pid=36573)\u001b[0m [4]\teval-rmse:2.17118\n",
      "\u001b[36m(my_xgboost_func pid=36573)\u001b[0m [5]\teval-rmse:2.16041\n",
      "\u001b[36m(my_xgboost_func pid=36566)\u001b[0m [0]\teval-rmse:2.21326\n",
      "\u001b[36m(my_xgboost_func pid=36573)\u001b[0m [6]\teval-rmse:2.15008\n",
      "\u001b[36m(my_xgboost_func pid=36566)\u001b[0m [1]\teval-rmse:2.19475\n",
      "\u001b[36m(my_xgboost_func pid=36573)\u001b[0m [7]\teval-rmse:2.14019\n",
      "\u001b[36m(my_xgboost_func pid=36566)\u001b[0m [2]\teval-rmse:2.17743\n",
      "\u001b[36m(my_xgboost_func pid=36566)\u001b[0m [3]\teval-rmse:2.16121\n",
      "\u001b[36m(my_xgboost_func pid=36573)\u001b[0m [8]\teval-rmse:2.13075\n",
      "\u001b[36m(my_xgboost_func pid=36566)\u001b[0m [4]\teval-rmse:2.14600\n",
      "\u001b[36m(my_xgboost_func pid=36573)\u001b[0m [9]\teval-rmse:2.12180\n",
      "\u001b[36m(my_xgboost_func pid=36566)\u001b[0m [5]\teval-rmse:2.13182\n",
      "\u001b[36m(my_xgboost_func pid=36566)\u001b[0m [6]\teval-rmse:2.11864\n",
      "\u001b[36m(my_xgboost_func pid=36566)\u001b[0m [7]\teval-rmse:2.10626\n",
      "\u001b[36m(my_xgboost_func pid=36566)\u001b[0m [8]\teval-rmse:2.09478\n",
      "\u001b[36m(my_xgboost_func pid=36566)\u001b[0m [9]\teval-rmse:2.08404\n",
      "\u001b[36m(my_xgboost_func pid=36564)\u001b[0m [0]\teval-rmse:2.21768\n",
      "\u001b[36m(my_xgboost_func pid=36564)\u001b[0m [1]\teval-rmse:2.20308\n",
      "\u001b[36m(my_xgboost_func pid=36577)\u001b[0m [0]\teval-rmse:2.16501\n",
      "\u001b[36m(my_xgboost_func pid=36564)\u001b[0m [2]\teval-rmse:2.18921\n",
      "\u001b[36m(my_xgboost_func pid=36577)\u001b[0m [1]\teval-rmse:2.11213\n",
      "\u001b[36m(my_xgboost_func pid=36564)\u001b[0m [3]\teval-rmse:2.17602\n",
      "\u001b[36m(my_xgboost_func pid=36564)\u001b[0m [4]\teval-rmse:2.16353\n",
      "\u001b[36m(my_xgboost_func pid=36577)\u001b[0m [2]\teval-rmse:2.07129\n",
      "\u001b[36m(my_xgboost_func pid=36577)\u001b[0m [3]\teval-rmse:2.03995\n",
      "\u001b[36m(my_xgboost_func pid=36564)\u001b[0m [5]\teval-rmse:2.15164\n",
      "\u001b[36m(my_xgboost_func pid=36577)\u001b[0m [4]\teval-rmse:2.01580\n",
      "\u001b[36m(my_xgboost_func pid=36564)\u001b[0m [6]\teval-rmse:2.14036\n",
      "\u001b[36m(my_xgboost_func pid=36577)\u001b[0m [5]\teval-rmse:1.99753\n",
      "\u001b[36m(my_xgboost_func pid=36564)\u001b[0m [7]\teval-rmse:2.12961\n",
      "\u001b[36m(my_xgboost_func pid=36564)\u001b[0m [8]\teval-rmse:2.11940\n",
      "\u001b[36m(my_xgboost_func pid=36577)\u001b[0m [6]\teval-rmse:1.98345\n",
      "\u001b[36m(my_xgboost_func pid=36577)\u001b[0m [7]\teval-rmse:1.97260\n",
      "\u001b[36m(my_xgboost_func pid=36564)\u001b[0m [9]\teval-rmse:2.10979\n",
      "\u001b[36m(my_xgboost_func pid=36577)\u001b[0m [8]\teval-rmse:1.96417\n",
      "\u001b[36m(my_xgboost_func pid=36577)\u001b[0m [9]\teval-rmse:1.95799\n",
      "\u001b[36m(my_xgboost_func pid=36571)\u001b[0m [0]\teval-rmse:2.13384\n",
      "\u001b[36m(my_xgboost_func pid=36571)\u001b[0m [1]\teval-rmse:2.06692\n",
      "\u001b[36m(my_xgboost_func pid=36571)\u001b[0m [2]\teval-rmse:2.02302\n",
      "\u001b[36m(my_xgboost_func pid=36571)\u001b[0m [3]\teval-rmse:1.99363\n",
      "\u001b[36m(my_xgboost_func pid=36571)\u001b[0m [4]\teval-rmse:1.97452\n",
      "\u001b[36m(my_xgboost_func pid=36571)\u001b[0m [5]\teval-rmse:1.96189\n",
      "\u001b[36m(my_xgboost_func pid=36571)\u001b[0m [6]\teval-rmse:1.95344\n",
      "\u001b[36m(my_xgboost_func pid=36571)\u001b[0m [7]\teval-rmse:1.94759\n",
      "\u001b[36m(my_xgboost_func pid=36571)\u001b[0m [8]\teval-rmse:1.94348\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m [0]\teval-rmse:2.13324\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m [1]\teval-rmse:2.06612\n",
      "\u001b[36m(my_xgboost_func pid=36571)\u001b[0m [9]\teval-rmse:1.94080\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m [2]\teval-rmse:2.02223\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m [3]\teval-rmse:1.99294\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m [4]\teval-rmse:1.97395\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m [5]\teval-rmse:1.96145\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m [6]\teval-rmse:1.95317\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m [7]\teval-rmse:1.94739\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m [8]\teval-rmse:1.94337\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m [9]\teval-rmse:1.94069\n",
      "\u001b[36m(my_xgboost_func pid=36561)\u001b[0m [0]\teval-rmse:2.17340\n",
      "\u001b[36m(my_xgboost_func pid=36569)\u001b[0m OrderedDict([('rmse', [2.1332434592091065, 2.0661233469523865, 2.022229703220378, 1.9929370385596794, 1.9739469695573346, 1.9614504493677505, 1.9531749379013732, 1.947387681277926, 1.9433684319451017, 1.9406899170422889])])\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(my_xgboost_func pid=36561)\u001b[0m [1]\teval-rmse:2.12549\n",
      "\u001b[36m(my_xgboost_func pid=36561)\u001b[0m [2]\teval-rmse:2.08683\n",
      "\u001b[36m(my_xgboost_func pid=36561)\u001b[0m [3]\teval-rmse:2.05630\n",
      "\u001b[36m(my_xgboost_func pid=36561)\u001b[0m [4]\teval-rmse:2.03172\n",
      "\u001b[36m(my_xgboost_func pid=36561)\u001b[0m [5]\teval-rmse:2.01241\n",
      "\u001b[36m(my_xgboost_func pid=36561)\u001b[0m [6]\teval-rmse:1.99687\n",
      "\u001b[36m(my_xgboost_func pid=36561)\u001b[0m [7]\teval-rmse:1.98460\n",
      "\u001b[36m(my_xgboost_func pid=36561)\u001b[0m [8]\teval-rmse:1.97490\n",
      "\u001b[36m(my_xgboost_func pid=36561)\u001b[0m [9]\teval-rmse:1.96708\n",
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m [0]\teval-rmse:2.22359\n",
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m [1]\teval-rmse:2.21441\n",
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m [2]\teval-rmse:2.20553\n",
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m [3]\teval-rmse:2.19692\n",
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m [4]\teval-rmse:2.18858\n",
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m [5]\teval-rmse:2.18048\n",
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m [6]\teval-rmse:2.17264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 23:43:25,284\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rbrosa/Documents/github_personal/intro-to-ray/intro/models/my_xgboost_func_2025-11-24_23-42-48' in 0.0110s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m [7]\teval-rmse:2.16505\n",
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m [8]\teval-rmse:2.15768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 23:43:25,288\tINFO tune.py:1041 -- Total run time: 30.24 seconds (29.29 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'hist', 'max_depth': 6, 'eta': 0.2807441797237359}\n"
     ]
    }
   ],
   "source": [
    "tuner = ray.tune.Tuner(  # Create a tuner\n",
    "    my_xgboost_func,  # Pass it the training function which Ray Tune calls Trainable.\n",
    "    param_space={  # Pass it the parameter space to search over\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"max_depth\": 6,\n",
    "        \"eta\": ray.tune.uniform(0.01, 0.3),\n",
    "    },\n",
    "    run_config=RunConfig(storage_path=storage_folder),\n",
    "    tune_config=ray.tune.TuneConfig(  # Tell it which metric to tune\n",
    "        metric=\"eval-rmse\",\n",
    "        mode=\"min\",\n",
    "        num_samples=10,\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()  # Run the tuning job\n",
    "print(results.get_best_result().config)  # Get back the best hyperparameters\n",
    "# About the below output table:\n",
    "# - it's dynamic: it has 1 row per task, with status updating as the jobs run (PENDING, RUNNING, FINISHED, FAILED)\n",
    "# - pid is the process id of the task (within a node of the cluster)\n",
    "# - ip is the ip address of the node where the task is running\n",
    "# - node_id is the id of the node where the task is running\n",
    "# - time_this_iter_s is the time taken by the task so far\n",
    "# - time_total_s is the total time taken by the task\n",
    "# - score is the score of the task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that shows what Tune does:\n",
    "\n",
    "It is effectively scheduling many trials and returning the best performing one.\n",
    "\n",
    "Obs.: the standard Trial Scheduler is FIFO (First-in-First-Out), but it could be alternatives more complex\n",
    "\n",
    "Obs.2: They don't mention how data memory is shared across nodes. Based on the image, I'd guess everything is replicated and the whole trainable is shared (but it's said they will deep diver in a another class)\n",
    "\n",
    "<img src=\"https://bair.berkeley.edu/static/blog/tune/tune-arch-simple.png\" width=\"700px\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Distributed training with Ray Train\n",
    "\n",
    "In case your training data is too large, your training might take a long time to complete.\n",
    "\n",
    "To speed it up, shard the dataset across training workers and perform distributed XGBoost training.\n",
    "\n",
    "Let's redefine `load_data` to now load a different slice of the data given the worker index/rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # find out which training worker is running this code (it will return a different context for each worker)\n",
    "    train_ctx = ray.train.get_context()\n",
    "    worker_rank = train_ctx.get_world_rank()\n",
    "    print(f\"Loading data for worker {worker_rank}...\")\n",
    "\n",
    "    # build path based on training worker rank\n",
    "    month = (worker_rank + 1) % 12\n",
    "    year = 2021 + (worker_rank + 1) // 12\n",
    "    path = f\"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_{year}-{month:02}.parquet\"\n",
    "\n",
    "    # same as before\n",
    "    df = pd.read_parquet(path, columns=features + [label_column])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features], df[label_column], test_size=0.2, random_state=42\n",
    "    )\n",
    "    dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run distributed XGBoost training using Ray Train's XGBoostTrainer - similar trainers exist for other popular ML frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 23:43:25,341\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m [9]\teval-rmse:2.15055\n",
      "\u001b[36m(my_xgboost_func pid=36570)\u001b[0m OrderedDict([('rmse', [2.223591551472562, 2.214411887253386, 2.205532735129033, 2.196918552330912, 2.188576980134632, 2.1804829498832183, 2.1726352475098256, 2.165048942296986, 2.1576837930141624, 2.150549162838675])])\n",
      "== Status ==\n",
      "Current time: 2025-11-24 23:43:25 (running for 00:00:00.13)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-11-24_23-42-48_935530_35114/artifacts/2025-11-24_23-43-25/XGBoostTrainer_2025-11-24_23-43-25/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-11-24 23:43:30 (running for 00:00:05.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-11-24_23-42-48_935530_35114/artifacts/2025-11-24_23-43-25/XGBoostTrainer_2025-11-24_23-43-25/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m - (node_id=19a03c4045d3f6add6eae2f8895430ab6b9819b97d746aa3363a095e, ip=127.0.0.1, pid=53418) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m - (node_id=19a03c4045d3f6add6eae2f8895430ab6b9819b97d746aa3363a095e, ip=127.0.0.1, pid=53414) world_rank=1, local_rank=1, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-11-24 23:43:35 (running for 00:00:10.25)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-11-24_23-42-48_935530_35114/artifacts/2025-11-24_23-43-25/XGBoostTrainer_2025-11-24_23-43-25/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=53418)\u001b[0m Loading data for worker 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=53418)\u001b[0m [23:43:35] Task [xgboost.ray-rank=00000000]:791668978711e8736a1fd8c401000000 got rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-11-24 23:43:40 (running for 00:00:15.31)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-11-24_23-42-48_935530_35114/artifacts/2025-11-24_23-43-25/XGBoostTrainer_2025-11-24_23-43-25/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-11-24 23:43:45 (running for 00:00:20.38)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-11-24_23-42-48_935530_35114/artifacts/2025-11-24_23-43-25/XGBoostTrainer_2025-11-24_23-43-25/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m [23:43:47] [0]\teval-rmse:2.28346\n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m [23:43:47] [1]\teval-rmse:2.25069\n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m [23:43:47] [2]\teval-rmse:2.22460\n",
      "\u001b[36m(RayTrainWorker pid=53414)\u001b[0m [23:43:35] Task [xgboost.ray-rank=00000001]:8e03f2ab335227788a0a0bf501000000 got rank 1\n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m [23:43:47] [3]\teval-rmse:2.20430\n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m [23:43:47] [4]\teval-rmse:2.18836\n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m [23:43:47] [5]\teval-rmse:2.17259\n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m [23:43:47] [6]\teval-rmse:2.15961\n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m [23:43:47] [7]\teval-rmse:2.14910\n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m [23:43:47] [8]\teval-rmse:2.14038\n",
      "\u001b[36m(XGBoostTrainer pid=50388)\u001b[0m [23:43:47] [9]\teval-rmse:2.13346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=53418)\u001b[0m OrderedDict([('rmse', [np.float64(2.283455977036048), np.float64(2.2506874095991365), np.float64(2.2245956490092267), np.float64(2.204302370137811), np.float64(2.1883569504768654), np.float64(2.1725859682897077), np.float64(2.1596106275006597), np.float64(2.149095348540642), np.float64(2.140382007941743), np.float64(2.133460611893429)])])\n",
      "\u001b[36m(RayTrainWorker pid=53414)\u001b[0m Loading data for worker 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 23:43:48,962\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/rbrosa/ray_results/XGBoostTrainer_2025-11-24_23-43-25' in 0.0043s.\n",
      "2025-11-24 23:43:48,964\tINFO tune.py:1041 -- Total run time: 23.62 seconds (23.61 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial XGBoostTrainer_83ebc_00000 completed. Last result: \n",
      "== Status ==\n",
      "Current time: 2025-11-24 23:43:48 (running for 00:00:23.61)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/10 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-11-24_23-42-48_935530_35114/artifacts/2025-11-24_23-43-25/XGBoostTrainer_2025-11-24_23-43-25/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={},\n",
       "  path='/Users/rbrosa/ray_results/XGBoostTrainer_2025-11-24_23-43-25/XGBoostTrainer_83ebc_00000_0_2025-11-24_23-43-25',\n",
       "  filesystem='local',\n",
       "  checkpoint=None\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = RayTrainXGBoostTrainer(  # Create a trainer\n",
    "    my_xgboost_func,  # Pass it the training function\n",
    "    scaling_config=ray.train.ScalingConfig(\n",
    "        num_workers=2, use_gpu=False\n",
    "    ),  # Define how many training workers\n",
    "    train_loop_config=params,  # Pass it the hyperparameters\n",
    ")\n",
    "\n",
    "trainer.fit()  # Run the training job\n",
    "# About the below output table:\n",
    "# - Look how it starts just stating the available CPUs/GPUs in the cluster\n",
    "# - Then it updates the status with resource usage update\n",
    "# - Lastly, it starts showing the Trainer logs per pid (look how it outputs the rank of the worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that shows what Train does:\n",
    "\n",
    "A train controller will create training workers and execute the training function on each worker.\n",
    "\n",
    "Ray Train delegates the distributed training to the underlying XGBoost framework. (again, data is owned by each worker)\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"700px\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Serving an ensemble model with Ray Serve\n",
    "\n",
    "Ray Serve allows for distributed serving of models and complex inference pipelines.\n",
    "\n",
    "Here is a diagram showing how to deploy an ensemble model with Ray Serve:\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/xjan103pcp94/3DJ7vVRxYIvcFO7JmIUMCx/77a45caa275ffa46f5135f4d6726dd4f/Figure_2_-_Fanout_and_ensemble.png\" width=\"700px\" loading=\"lazy\">\n",
    "\n",
    "Here is how the resulting code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING 2025-11-24 23:43:49,001 serve 35114 -- There are multiple deployments with the same name 'Model'. Renaming one to 'Model_1'.\n",
      "\u001b[36m(ProxyActor pid=62784)\u001b[0m INFO 2025-11-24 23:43:50,690 proxy 127.0.0.1 -- Proxy starting on node 19a03c4045d3f6add6eae2f8895430ab6b9819b97d746aa3363a095e (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=62784)\u001b[0m INFO 2025-11-24 23:43:50,753 proxy 127.0.0.1 -- Got updated endpoints: {}.\n",
      "INFO 2025-11-24 23:43:50,801 serve 35114 -- Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ServeController pid=49514)\u001b[0m INFO 2025-11-24 23:43:50,870 controller 49514 -- Deploying new version of Deployment(name='Model', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=49514)\u001b[0m INFO 2025-11-24 23:43:50,871 controller 49514 -- Deploying new version of Deployment(name='Model_1', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=49514)\u001b[0m INFO 2025-11-24 23:43:50,872 controller 49514 -- Deploying new version of Deployment(name='Ensemble', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ProxyActor pid=62784)\u001b[0m INFO 2025-11-24 23:43:50,876 proxy 127.0.0.1 -- Got updated endpoints: {Deployment(name='Ensemble', app='default'): EndpointInfo(route='/ensemble', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=62784)\u001b[0m INFO 2025-11-24 23:43:50,882 proxy 127.0.0.1 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x1250e18d0>.\n",
      "\u001b[36m(ServeController pid=49514)\u001b[0m INFO 2025-11-24 23:43:50,988 controller 49514 -- Adding 1 replica to Deployment(name='Model', app='default').\n",
      "\u001b[36m(ServeController pid=49514)\u001b[0m INFO 2025-11-24 23:43:50,990 controller 49514 -- Adding 1 replica to Deployment(name='Model_1', app='default').\n",
      "\u001b[36m(ServeController pid=49514)\u001b[0m INFO 2025-11-24 23:43:50,991 controller 49514 -- Adding 1 replica to Deployment(name='Ensemble', app='default').\n",
      "INFO 2025-11-24 23:43:53,961 serve 35114 -- Application 'default' is ready at http://127.0.0.1:8000/ensemble.\n"
     ]
    }
   ],
   "source": [
    "app = fastapi.FastAPI()\n",
    "\n",
    "class Payload(BaseModel):\n",
    "    passenger_count: int\n",
    "    trip_distance: float\n",
    "    fare_amount: float\n",
    "    tolls_amount: float\n",
    "\n",
    "\n",
    "## Decorator to deploy the ensemble model\n",
    "# @ray.serve.deployment: decorator to deploy the ensemble model\n",
    "@ray.serve.deployment\n",
    "# @ray.serve.ingress(app): decorator to ingest the FastAPI app into the ray serve instance\n",
    "@ray.serve.ingress(app)\n",
    "class Ensemble:\n",
    "    def __init__(self, model1, model2):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    @app.post(\"/predict\")\n",
    "    async def predict(self, data: Payload) -> dict:\n",
    "        model1_prediction, model2_prediction = await asyncio.gather(\n",
    "            self.model1.predict.remote([data.model_dump()]),\n",
    "            self.model2.predict.remote([data.model_dump()]),\n",
    "        )\n",
    "        out = {\"prediction\": float(model1_prediction + model2_prediction) / 2}\n",
    "        return out\n",
    "\n",
    "\n",
    "@ray.serve.deployment\n",
    "class Model:\n",
    "    def __init__(self, path: str):\n",
    "        self._model = xgboost.Booster()\n",
    "        self._model.load_model(path)\n",
    "\n",
    "    def predict(self, data: list[dict]) -> list[float]:\n",
    "        # Make prediction\n",
    "        dmatrix = xgboost.DMatrix(pd.DataFrame(data))\n",
    "        model_prediction = self._model.predict(dmatrix)\n",
    "        return model_prediction\n",
    "\n",
    "\n",
    "# Run the deployment\n",
    "handle = ray.serve.run(\n",
    "    Ensemble.bind(\n",
    "        model1=Model.bind(model_path),\n",
    "        model2=Model.bind(model_path),\n",
    "    ),\n",
    "    route_prefix=\"/ensemble\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an HTTP request to the Ray Serve instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 2.0076115131378174}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(\n",
    "    \"http://localhost:8000/ensemble/predict\",\n",
    "    json={  # Use json parameter instead of params\n",
    "        \"passenger_count\": 1,\n",
    "        \"trip_distance\": 2.5,\n",
    "        \"fare_amount\": 10.0,\n",
    "        \"tolls_amount\": 0.5,\n",
    "    },\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Batch inference with Ray Data\n",
    "\n",
    "Ray Data allows for distributed data processing through streaming execution across a heterogeneous cluster of CPUs and GPUs.\n",
    "\n",
    "This makes Ray Data ideal for workloads like compute-intensive data processing, data ingestion, and batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:Model pid=63277)\u001b[0m /Users/rbrosa/Documents/github_personal/intro-to-ray/.venv/lib/python3.11/site-packages/ray/serve/_private/replica.py:1339: UserWarning: Calling sync method 'predict' directly on the asyncio loop. In a future version, sync methods will be run in a threadpool by default. Ensure your sync methods are thread safe or keep the existing behavior by making them `async def`. Opt into the new behavior by setting RAY_SERVE_RUN_SYNC_IN_THREADPOOL=1.\n",
      "\u001b[36m(ServeReplica:default:Model pid=63277)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ServeReplica:default:Model pid=63277)\u001b[0m INFO 2025-11-24 23:43:54,010 default_Model h65kr9da f2712793-2982-4e97-83d2-4eda2e924df0 -- CALL /ensemble/predict OK 5.6ms\n",
      "\u001b[36m(ServeReplica:default:Model_1 pid=63284)\u001b[0m INFO 2025-11-24 23:43:54,013 default_Model_1 e8yj62gq f2712793-2982-4e97-83d2-4eda2e924df0 -- CALL /ensemble/predict OK 5.4ms\n",
      "\u001b[36m(ServeReplica:default:Ensemble pid=63291)\u001b[0m INFO 2025-11-24 23:43:53,993 default_Ensemble 16tlnz25 f2712793-2982-4e97-83d2-4eda2e924df0 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x12e3517d0>.\n",
      "\u001b[36m(ServeReplica:default:Ensemble pid=63291)\u001b[0m /var/folders/2z/npzthnz14pqcwxlslfpbm8xr0000gp/T/ipykernel_35114/1272336709.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "\u001b[36m(ServeReplica:default:Ensemble pid=63291)\u001b[0m INFO 2025-11-24 23:43:54,015 default_Ensemble 16tlnz25 f2712793-2982-4e97-83d2-4eda2e924df0 -- POST /ensemble/predict 200 34.8ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e395c311d5549a59b854e20cb67f97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parquet Files Sample 0:   0%|          | 0.00/1.00 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class OfflinePredictor:\n",
    "    def __init__(self):\n",
    "        # Load expensive state\n",
    "        self._model = xgboost.Booster()\n",
    "        self._model.load_model(model_path)\n",
    "\n",
    "    def predict(self, data: list[dict]) -> list[float]:\n",
    "        # Make prediction in batch\n",
    "        dmatrix = xgboost.DMatrix(pd.DataFrame(data))\n",
    "        model_prediction = self._model.predict(dmatrix)\n",
    "        return model_prediction\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        batch[\"predictions\"] = self.predict(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Apply the predictor to the validation dataset\n",
    "prediction_pipeline = (\n",
    "    ray.data.read_parquet(\n",
    "        \"s3://anyscale-public-materials/nyc-taxi-cab/yellow_tripdata_2021-03.parquet\"\n",
    "    )\n",
    "    .select_columns(features)\n",
    "    .map_batches(OfflinePredictor, concurrency=(2, 10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the pipeline, we can execute it in a distributed manner by writing the output to a sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 23:44:10,964\tINFO logging.py:290 -- Registered dataset logger for dataset dataset_4_0\n",
      "2025-11-24 23:44:10,977\tINFO streaming_executor.py:117 -- Starting execution of Dataset dataset_4_0. Full logs are in /tmp/ray/session_2025-11-24_23-42-48_935530_35114/logs/ray-data\n",
      "2025-11-24 23:44:10,977\tINFO streaming_executor.py:118 -- Execution plan of Dataset dataset_4_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> ActorPoolMapOperator[Project->MapBatches(OfflinePredictor)] -> TaskPoolMapOperator[Write]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6aba9bec4248f7abffd2eb5daf33c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763d707da22e4f68b15bb6d6a6820c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadParquet->SplitBlocks(100) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70871b9ad2ab44eb81dc922d4f2c5b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Project->MapBatches(OfflinePredictor) 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2630fc7c454aeaa8188cdf068c8bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Write 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 23:45:01,158\tINFO streaming_executor.py:220 -- ‚úîÔ∏è  Dataset dataset_4_0 execution finished in 50.18 seconds\n",
      "2025-11-24 23:45:01,225\tINFO dataset.py:4537 -- Data sink Parquet finished. 1925152 rows and 66.1MB data written.\n"
     ]
    }
   ],
   "source": [
    "prediction_pipeline.write_parquet(\"./xgboost_predictions\") #update this to your local path if runs on your local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the produced predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /Users/rbrosa/Documents/github_personal/intro-to-ray/intro/models/xgboost_predictions/: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls {storage_folder}/xgboost_predictions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:Ensemble pid=63291)\u001b[0m INFO 2025-11-25 00:21:22,538 default_Ensemble 16tlnz25 69b644e2-55ea-4285-9e86-396a6175ec0d -- GET /ensemble 405 2.8ms\n"
     ]
    }
   ],
   "source": [
    "# Run this cell for file cleanup \n",
    "!rm -rf {storage_folder}/xgboost_predictions/\n",
    "!rm {model_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
